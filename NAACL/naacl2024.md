# naacl2024 - Jailbreak Research Papers

**Total Papers:** 6

## Content

- [Attack](#Attack)
  - [Text](#Attack-Text)
- [Defense](#Defense)
  - [Text](#Defense-Text)
- [Benchmark](#Benchmark)
  - [Text](#Benchmark-Text)

---

## Attack

### Text

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| A Wolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily | Nanjing University | main | 0.0 | Long | [ðŸ”—](https://aclanthology.org/2024.naacl-long.118/) |
| Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking | USC | main | 0.0 | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-naacl.224/) |


## Defense

### Text

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Advancing the Robustness of Large Language Models through Self-Denoised Smoothing | N/A | main | 0.0 | Short | [ðŸ”—](https://aclanthology.org/2024.naacl-short.23/) |
| SELF-GUARD: Empower the LLM to Safeguard Itself | CUHK | main | 0.0 | Long | [ðŸ”—](https://aclanthology.org/2024.naacl-long.92/) |


## Benchmark

### Text

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Fake Alignment: Are LLMs Really Aligned Well? | Fudan | main | 0.0 | Long | [ðŸ”—](https://aclanthology.org/2024.naacl-long.263/) |
| Flames: Benchmarking Value Alignment of LLMs in Chinese | Shanghai AI Lab | main | 0.0 | Long | [ðŸ”—](https://aclanthology.org/2024.naacl-long.256/) |


