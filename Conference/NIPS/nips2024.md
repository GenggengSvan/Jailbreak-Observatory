# nips2024 - Jailbreak Research Papers

**Total Papers:** 33

## Content

- [Attack](#Attack)
  - [Text](#Attack-Text)
  - [Hybrid](#Attack-Hybrid)
- [Defense](#Defense)
  - [Text](#Defense-Text)
  - [Vision](#Defense-Vision)
  - [Hybrid](#Defense-Hybrid)
- [Benchmark](#Benchmark)
  - [Text](#Benchmark-Text)
  - [Vision](#Benchmark-Vision)
  - [Hybrid](#Benchmark-Hybrid)
- [Mechanism](#Mechanism)
  - [Text](#Mechanism-Text)

---

## Attack

### Text (9 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Many-shot Jailbreaking | U of T | main | 6.4 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94370) |
| Tree of Attacks: Jailbreaking Black-Box LLMs Automatically | Yale | main | 6.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/95078) |
| Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters | ZJUT | main | 6.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/96243) |
| Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses | N/A | main | 5.8 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/92958) |
| When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search | Purdue | main | 5.8 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/95953) |
| WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models | UW | main | 5.8 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/93716) |
| Improved Generation of Adversarial Examples Against Safety-aligned LLMs | HIT | main | 5.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/96362) |
| Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization | CMU | main | 5.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94489) |
| Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space | TUM | main | 5.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/96148) |


### Hybrid (1 paper)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| ColJailBreak: Collaborative Generation and Editing for Jailbreaking Text-to-Image Deep Generation | XJTU | main | 5.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94287) |


## Defense

### Text (11 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| WildGuard: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs | SNU | Datasets & Benchmarks | 7.3 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/97764) |
| Time-Reversal Provides Unsupervised Feedback to LLMs | DeepMind | main | 7.3 | Spotlight | [ðŸ”—](https://neurips.cc/virtual/2024/poster/93684) |
| Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks | N/A | main | 7.0 | Spotlight | [ðŸ”—](https://neurips.cc/virtual/2024/poster/93953) |
| A Theoretical Understanding of Self-Correction through In-context Alignment | MIT | main | 6.6 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/95342) |
| Protecting Your LLMs with Information Bottleneck | Nanjing U | main | 6.5 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/93290) |
| Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization | PSU | main | 6.3 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/96424) |
| BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment | UW-Madison | main | 6.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/96865) |
| TAIA: Large Language Models are Out-of-Distribution Data Learners | Fudan | main | 6.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94733) |
| Representation Noising: A Defence Mechanism Against Harmful Finetuning | scite.ai | main | 5.8 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94272) |
| Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes | CUHK | main | 5.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/93211) |
| Fight Back Against Jailbreaking via Prompt Adversarial Tuning | Peking U | main | 4.8 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/93692) |


### Vision (1 paper)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models | SEU | main | 6.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94704) |


### Hybrid (1 paper)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| ProTransformer: Robustify Transformers via Plug-and-Play Paradigm | Amazon | main | 5.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94947) |


## Benchmark

### Text (4 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| A StrongREJECT for Empty Jailbreaks | UK AMO | Datasets & Benchmarks | 7.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/97752) |
| SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types | Peking U | Datasets & Benchmarks | 6.8 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/97610) |
| Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs | HKUST | Datasets & Benchmarks | 6.5 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/97431) |
| JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | UPenn Wharton | Datasets & Benchmarks | 6.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/97459) |


### Vision (2 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models | THU | Datasets & Benchmarks | 6.8 | Poster | [ðŸ”—](https://openreview.net/forum?id=5c1hh8AeHv) |
| T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models | Intel | Datasets & Benchmarks | 6.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/97732) |


### Hybrid (1 paper)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users | NTU | main | 5.2 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/95859) |


## Mechanism

### Text (3 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Refusal in Language Models Is Mediated by a Single Direction | UChicago | main | 6.5 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/93566) |
| Mission Impossible: A Statistical Perspective on Jailbreaking LLMs | NYU | main | 6.0 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/94247) |
| What Makes and Breaks Safety Fine-tuning? A Mechanistic Study | Five AI | main | 5.8 | Poster | [ðŸ”—](https://neurips.cc/virtual/2024/poster/95726) |


