# acl2025 - Jailbreak Research Papers

**Total Papers:** 59

## Content

- [Attack](#Attack)
  - [Text](#Attack-Text)
  - [Vision](#Attack-Vision)
  - [Hybrid](#Attack-Hybrid)
  - [Agent](#Attack-Agent)
- [Defense](#Defense)
  - [Text](#Defense-Text)
  - [Vision](#Defense-Vision)
- [Benchmark](#Benchmark)
  - [Text](#Benchmark-Text)
- [Mechanism](#Mechanism)
  - [Text](#Mechanism-Text)

---

## Attack

### Text 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.38/) |
| Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.965/) |
| Jailbreaking? One Step Is Enough! | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.570/) |
| M2S: Multi-turn to Single-turn jailbreak in Red Teaming for LLMs | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.805/) |
| PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.475/) |
| Sheepâ€™s Skin, Wolfâ€™s Deeds: Are LLMs Ready for Metaphorical Implicit Hate Speech? | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.814/) |
| The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.334/) |
| What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.101/) |
| from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.238/) |
| Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.410/) |
| CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.346/) |
| Chain of Attack: Hide Your Intention through Multi-Turn Interrogation | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.514/) |
| Donâ€™t Say No: Jailbreaking LLM by Suppressing Refusal | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1294/) |
| Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1067/) |
| GOODLIAR: A Reinforcement Learning-Based Deceptive Agent for Disrupting LLM Beliefs on Foundational Principles | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.160/) |
| QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.298/) |
| Red Queen: Exposing Latent Multi-Turn Risks in Large Language Models | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1311/) |
| Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.189/) |
| SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.100/) |
| SQL Injection Jailbreak: A Structural Disaster of Large Language Models | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.358/) |
| SemanticCamo: Jailbreaking Large Language Models through Semantic Camouflage | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.745/) |
| The Threat of PROMPTS in Large Language Models: A System and User Prompt Perspective | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.675/) |


### Vision 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.146/) |
| Jailbreak Large Vision-Language Models Through Multi-Modal Linkage | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.74/) |
| Multimodal Pragmatic Jailbreak on Text-to-image Models | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.234/) |
| Chain-of-Jailbreak Attack for Image Generation Models via Step by Step Editing | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.571/) |


### Hybrid 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.408/) |
| The Structural Safety Generalization Problem | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1142/) |


### Agent 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.859/) |
| Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.476/) |


## Defense

### Text 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| AGD: Adversarial Game Defense Against Jailbreak Attacks in Large Language Models | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.851/) |
| DeAL: Decoding-time Alignment for Large Language Models | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1274/) |
| Improve Safety Training of Large Language Models with Safety-Critical Singular Vectors Localization | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.245/) |
| MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1282/) |
| Mixture of insighTful Experts (MoTE): The Synergy of Reasoning Chains and Expert Mixtures in Self-Alignment | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.151/) |
| Representation Bending for Large Language Model Safety | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1173/) |
| Root Defense Strategies: Ensuring Safety of LLM at the Decoding Level | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.97/) |
| Safety Alignment via Constrained Knowledge Unlearning | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1240/) |
| TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.733/) |
| Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1013/) |
| Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.760/) |
| DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.598/) |
| DIESEL: A Lightweight Inference-Time Safety Enhancement for Language Models | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1223/) |
| DeTAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.613/) |
| Defensive Prompt Patch: A Robust and Generalizable Defense of Large Language Models against Jailbreak Attacks | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.23/) |
| From Evasion to Concealment: Stealthy Knowledge Unlearning for LLMs | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.535/) |
| SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1197/) |
| Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.960/) |
| Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.66/) |
| Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.1166/) |
| Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.325/) |


### Vision 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| HiddenDetect: Detecting Jailbreak Attacks against Multimodal Large Language Models via Monitoring Hidden States | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.724/) |


## Benchmark

### Text 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1045/) |
| LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1350/) |
| Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.885/) |


## Mechanism

### Text 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1233/) |
| Small Changes, Big Impact: How Manipulating a Few Neurons Can Drastically Alter LLM Aggression | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.1144/) |
| Why Safeguarded Ships Run Aground? Aligned Large Language Modelsâ€™ Safety Mechanisms Tend to Be Anchored in The Template Region | N/A | main | Long | [ðŸ”—](https://aclanthology.org/2025.acl-long.738/) |
| Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking? | N/A | main | finding | [ðŸ”—](https://aclanthology.org/2025.findings-acl.339/) |


