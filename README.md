# Jailbreak-Observatory
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/GenggengSvan/Jailbreak-Observatory)
[![GitHub Repo stars](https://img.shields.io/github/stars/GenggengSvan/Jailbreak-Observatory)](https://github.com/GenggengSvan/Jailbreak-Observatory)
<img src="https://img.shields.io/badge/Jailbreak-green">

ðŸ“Š The [acceptance status of papers related to Jailbreak research.](#document-list-by-conference)

ðŸ“‘ The [top 100 most cited Jailbreak articles.](#cite)

---
<a id="document-list-by-conference"></a>
## ðŸ“Š Document List by Conference

<table>
<tr><th style='text-align:center'>Conference</th><th style='text-align:center'>Year</th><th style='text-align:center'>Document</th><th style='text-align:center'>Papers Count</th></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='3'><strong>NIPS</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/NIPS/nips2025.md'>NIPS2025</a></td><td style='text-align:center'>37</td></tr>
<tr><td style='text-align:center'>2024</td><td><a href='Conference/NIPS/nips2024.md'>NIPS2024</a></td><td style='text-align:center'>33</td></tr>
<tr><td style='text-align:center'>2023</td><td><a href='Conference/NIPS/nips2023.md'>NIPS2023</a></td><td style='text-align:center'>2</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='2'><strong>ICLR</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/ICLR/iclr2025.md'>ICLR2025</a></td><td style='text-align:center'>35</td></tr>
<tr><td style='text-align:center'>2024</td><td><a href='Conference/ICLR/iclr2024.md'>ICLR2024</a></td><td style='text-align:center'>8</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='2'><strong>ICML</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/ICML/icml2025.md'>ICML2025</a></td><td style='text-align:center'>23</td></tr>
<tr><td style='text-align:center'>2024</td><td><a href='Conference/ICML/icml2024.md'>ICML2024</a></td><td style='text-align:center'>10</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='2'><strong>AAAI</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/AAAI/aaai2025.md'>AAAI2025</a></td><td style='text-align:center'>17</td></tr>
<tr><td style='text-align:center'>2024</td><td><a href='Conference/AAAI/aaai2024.md'>AAAI2024</a></td><td style='text-align:center'>1</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='2'><strong>ACL</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/ACL/acl2025.md'>ACL2025</a></td><td style='text-align:center'>59</td></tr>
<tr><td style='text-align:center'>2024</td><td><a href='Conference/ACL/acl2024.md'>ACL2024</a></td><td style='text-align:center'>18</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='2'><strong>EMNLP</strong></td><td style='text-align:center'>2024</td><td><a href='Conference/EMNLP/emnlp2024.md'>EMNLP2024</a></td><td style='text-align:center'>20</td></tr>
<tr><td style='text-align:center'>2023</td><td><a href='Conference/EMNLP/emnlp2023.md'>EMNLP2023</a></td><td style='text-align:center'>4</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='2'><strong>NAACL</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/NAACL/naacl2025.md'>NAACL2025</a></td><td style='text-align:center'>26</td></tr>
<tr><td style='text-align:center'>2024</td><td><a href='Conference/NAACL/naacl2024.md'>NAACL2024</a></td><td style='text-align:center'>6</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='1'><strong>WWW</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/WWW/www2025.md'>WWW2025</a></td><td style='text-align:center'>2</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='2'><strong>SP</strong></td><td style='text-align:center'>2025</td><td><a href='Conference/SP/sp2025.md'>SP2025</a></td><td style='text-align:center'>2</td></tr>
<tr><td style='text-align:center'>2024</td><td><a href='Conference/SP/sp2024.md'>SP2024</a></td><td style='text-align:center'>1</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='1'><strong>CCS</strong></td><td style='text-align:center'>2024</td><td><a href='Conference/CCS/ccs2024.md'>CCS2024</a></td><td style='text-align:center'>1</td></tr>
<tr><td style='text-align:center;vertical-align:middle' rowspan='1'><strong>NDSS</strong></td><td style='text-align:center'>2024</td><td><a href='Conference/NDSS/ndss2024_fall.md'>NDSS2024_FALL</a></td><td style='text-align:center'>1</td></tr>
</table>


## ðŸ“‹ Legend

- **Conference**: Academic conference (NIPS, ICLR, etc.)
- **Papers Count**: Number of papers collection documents in this conference
- **Document**: Each link points to a Markdown file containing classified and analyzed research papers

#### Document Content
Each document typically contains:
- Paper title and metadata
- Classification (Attack/Defense/Benchmark/Mechanism/Other)
- LLM type (Text/Vision/Hybrid/Agent)
- Average rating scores
- Direct links to original papers

---
<a id="cite"></a>
## ðŸ“‘ Top 100 Citations

## Summary Table
For full details, see the citations document <a href='citations_top_100.md'>Citations Top 100</a>.

<table>
  <thead><tr><th>Target</th><th>Category</th><th>Title</th><th>Citations</th></tr></thead>
  <tbody>
  <tr>    <td rowspan="76">Text</td>    <td rowspan="37">Attack</td>    <td>AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</td>    <td>526</td></tr>
  <tr>    <td>GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts</td>    <td>481</td></tr>
  <tr>    <td>How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs</td>    <td>459</td></tr>
  <tr>    <td>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation</td>    <td>390</td></tr>
  <tr>    <td>Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations</td>    <td>383</td></tr>
  <tr>    <td>Low-Resource Languages Jailbreak GPT-4</td>    <td>261</td></tr>
  <tr>    <td>A Wolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily</td>    <td>202</td></tr>
  <tr>    <td>Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots</td>    <td>187</td></tr>
  <tr>    <td>ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</td>    <td>182</td></tr>
  <tr>    <td>Universal Jailbreak Backdoors from Poisoned Human Feedback</td>    <td>106</td></tr>
  <tr>    <td>AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs</td>    <td>84</td></tr>
  <tr>    <td>Don&#x27;t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models</td>    <td>78</td></tr>
  <tr>    <td>Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning</td>    <td>66</td></tr>
  <tr>    <td>Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues</td>    <td>64</td></tr>
  <tr>    <td>FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models</td>    <td>56</td></tr>
  <tr>    <td>Distract Large Language Models for Automatic Jailbreak Attack</td>    <td>44</td></tr>
  <tr>    <td>Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs</td>    <td>43</td></tr>
  <tr>    <td>FlipAttack: Jailbreak LLMs via Flipping</td>    <td>38</td></tr>
  <tr>    <td>Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles</td>    <td>38</td></tr>
  <tr>    <td>All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks</td>    <td>31</td></tr>
  <tr>    <td>ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings</td>    <td>28</td></tr>
  <tr>    <td>Boosting Jailbreak Attack with Momentum</td>    <td>26</td></tr>
  <tr>    <td>WordGame: Efficient &amp; Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response</td>    <td>26</td></tr>
  <tr>    <td>Jailbreak Open-Sourced Large Language Models via Enforced Decoding</td>    <td>24</td></tr>
  <tr>    <td>Perception-guided Jailbreak against Text-to-Image Models</td>    <td>24</td></tr>
  <tr>    <td>Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs</td>    <td>23</td></tr>
  <tr>    <td>MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue</td>    <td>21</td></tr>
  <tr>    <td>Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection</td>    <td>21</td></tr>
  <tr>    <td>Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation</td>    <td>18</td></tr>
  <tr>    <td>Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization</td>    <td>17</td></tr>
  <tr>    <td>Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation</td>    <td>13</td></tr>
  <tr>    <td>AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens</td>    <td>13</td></tr>
  <tr>    <td>PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach</td>    <td>13</td></tr>
  <tr>    <td>Poisoned LangChain: Jailbreak LLMs by LangChain</td>    <td>13</td></tr>
  <tr>    <td>Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models</td>    <td>12</td></tr>
  <tr>    <td>Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks</td>    <td>11</td></tr>
  <tr>    <td>BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting</td>    <td>10</td></tr>
  <tr>    <td rowspan="22">Defense</td>    <td>Defending ChatGPT against jailbreak attack via self-reminders</td>    <td>337</td></tr>
  <tr>    <td>Multilingual Jailbreak Challenges in Large Language Models</td>    <td>186</td></tr>
  <tr>    <td>SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</td>    <td>179</td></tr>
  <tr>    <td>Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing</td>    <td>69</td></tr>
  <tr>    <td>GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis</td>    <td>58</td></tr>
  <tr>    <td>Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing</td>    <td>52</td></tr>
  <tr>    <td>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes</td>    <td>48</td></tr>
  <tr>    <td>Intention Analysis Makes LLMs A Good Jailbreak Defender</td>    <td>48</td></tr>
  <tr>    <td>Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender</td>    <td>45</td></tr>
  <tr>    <td>Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks</td>    <td>32</td></tr>
  <tr>    <td>Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment</td>    <td>32</td></tr>
  <tr>    <td>Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment</td>    <td>28</td></tr>
  <tr>    <td>Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning</td>    <td>28</td></tr>
  <tr>    <td>Defending Jailbreak Prompts via In-Context Adversarial Game</td>    <td>26</td></tr>
  <tr>    <td>Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement</td>    <td>25</td></tr>
  <tr>    <td>Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs</td>    <td>23</td></tr>
  <tr>    <td>BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment</td>    <td>23</td></tr>
  <tr>    <td>From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending Against Jailbreak Attacks</td>    <td>17</td></tr>
  <tr>    <td>RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction</td>    <td>15</td></tr>
  <tr>    <td>SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance</td>    <td>12</td></tr>
  <tr>    <td>HSF: Defending against Jailbreak Attacks with Hidden State Filtering</td>    <td>11</td></tr>
  <tr>    <td>Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models</td>    <td>11</td></tr>
  <tr>    <td rowspan="10">Benchmark</td>    <td>&quot;Do Anything Now&quot;: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</td>    <td>426</td></tr>
  <tr>    <td>A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models</td>    <td>83</td></tr>
  <tr>    <td>JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs</td>    <td>79</td></tr>
  <tr>    <td>Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models</td>    <td>60</td></tr>
  <tr>    <td>Comprehensive Assessment of Jailbreak Attacks Against LLMs</td>    <td>41</td></tr>
  <tr>    <td>JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework</td>    <td>33</td></tr>
  <tr>    <td>Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs</td>    <td>24</td></tr>
  <tr>    <td>AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models</td>    <td>22</td></tr>
  <tr>    <td>JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models</td>    <td>21</td></tr>
  <tr>    <td>&quot;Not Aligned&quot; is Not &quot;Malicious&quot;: Being Careful about Hallucinations of Large Language Models&#x27; Jailbreak</td>    <td>16</td></tr>
  <tr>    <td rowspan="5">Mechanism</td>    <td>How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States</td>    <td>71</td></tr>
  <tr>    <td>Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis</td>    <td>41</td></tr>
  <tr>    <td>Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models</td>    <td>27</td></tr>
  <tr>    <td>What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks</td>    <td>17</td></tr>
  <tr>    <td>JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit</td>    <td>15</td></tr>
  <tr>    <td rowspan="2">Other</td>    <td>A Cross-Language Investigation into Jailbreak Attacks in Large Language Models</td>    <td>37</td></tr>
  <tr>        <td>A Cross-Language Investigation into Jailbreak Attacks in Large Language Models</td>    <td>37</td></tr>
  <tr>    <td rowspan="15">Vision</td>    <td rowspan="7">Attack</td>    <td>Visual Adversarial Examples Jailbreak Aligned Large Language Models</td>    <td>258</td></tr>
  <tr>    <td>Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</td>    <td>217</td></tr>
  <tr>    <td>Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt</td>    <td>71</td></tr>
  <tr>    <td>Query-Relevant Images Jailbreak Large Multi-Modal Models</td>    <td>52</td></tr>
  <tr>    <td>Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Characte</td>    <td>52</td></tr>
  <tr>    <td>Jailbreak Large Vision-Language Models Through Multi-Modal Linkage</td>    <td>28</td></tr>
  <tr>    <td>Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models</td>    <td>19</td></tr>
  <tr>    <td rowspan="5">Defense</td>    <td>Defending Jailbreak Attack in VLMs via Cross-modality Information Detector</td>    <td>18</td></tr>
  <tr>    <td>BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks</td>    <td>15</td></tr>
  <tr>    <td>UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models</td>    <td>12</td></tr>
  <tr>    <td>Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks</td>    <td>11</td></tr>
  <tr>    <td>BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger</td>    <td>10</td></tr>
  <tr>    <td rowspan="2">Benchmark</td>    <td>JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks</td>    <td>157</td></tr>
  <tr>    <td>Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts</td>    <td>34</td></tr>
  <tr>    <td rowspan="1">Mechanism</td>    <td>The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense</td>    <td>15</td></tr>
  <tr>    <td rowspan="8">Hybrid</td>    <td rowspan="4">Attack</td>    <td>Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack</td>    <td>181</td></tr>
  <tr>    <td>Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast</td>    <td>91</td></tr>
  <tr>    <td>Voice Jailbreak Attacks Against GPT-4o</td>    <td>24</td></tr>
  <tr>    <td>AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models</td>    <td>18</td></tr>
  <tr>    <td rowspan="4">Benchmark</td>    <td>LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study</td>    <td>67</td></tr>
  <tr>    <td>Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak Attacks</td>    <td>38</td></tr>
  <tr>    <td>Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?</td>    <td>31</td></tr>
  <tr>    <td>Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey</td>    <td>22</td></tr>
  <tr>    <td rowspan="1">Agent</td>    <td rowspan="1">Defense</td>    <td>AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks</td>    <td>117</td></tr>
  </tbody>
</table>
