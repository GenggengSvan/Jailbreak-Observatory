# iclr2025 - Jailbreak Research Papers

**Total Papers:** 35

## Content

- [Attack](#Attack)
  - [Text](#Attack-Text)
  - [Vision](#Attack-Vision)
  - [Hybrid](#Attack-Hybrid)
- [Defense](#Defense)
  - [Text](#Defense-Text)
  - [Vision](#Defense-Vision)
- [Benchmark](#Benchmark)
  - [Text](#Benchmark-Text)
  - [Agent](#Benchmark-Agent)
- [Mechanism](#Mechanism)
  - [Text](#Mechanism-Text)

---

## Attack

### Text (17 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs | UW-Madison | main | 7.2 | Spotlight | [ðŸ”—](https://iclr.cc/virtual/2025/poster/29096) |
| One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs | HIT | main | 7.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28127) |
| Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks | UW | main | 7.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27963) |
| h4rm3l: A Language for Composable Jailbreak Attack Synthesis | Stanford | main | 6.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27663) |
| Injecting Universal Jailbreak Backdoors into LLMs in Minutes | GDUFS | main | 6.7 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/29167) |
| Improved Techniques for Optimization-Based Jailbreaking on Large Language Models | NTU | main | 6.2 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28946) |
| EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING | Fujitsu R&D | main | 6.2 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28647) |
| Endless Jailbreaks with Bijection Learning | N/A | main | 6.2 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27786) |
| Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks | EPFL | main | 6.1 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28755) |
| Durable Quantization Conditioned Misalignment Attack on Large Language Models | PolyU | main | 6.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/31042) |
| Understanding and Enhancing the Transferability of Jailbreaking Attacks | USYD | main | 6.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/29142) |
| Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing | CMU SEI | main | 5.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/29632) |
| Does Refusal Training in LLMs Generalize to the Past Tense? | EPFL | main | 5.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/29179) |
| Persistent Pre-training Poisoning of LLMs | CMU | main | 5.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28909) |
| Jailbreaking as a Reward Misspecification Problem | HKU | main | 5.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27994) |
| Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts? | DeepMind | main | 5.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/30001) |
| ProAdvPrompter: A Two-Stage Journey to Effective Adversarial Prompting for LLMs | XJTU | main | 5.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28021) |


### Vision (1 paper)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Failures to Find Transferable Image Jailbreaks Between Vision-Language Models | Stanford | main | 6.2 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27813) |


### Hybrid (2 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| BadRobot: Jailbreaking Embodied LLM Agents in the Physical World | HUST | main | 6.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/32071) |
| AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models | UIUC | main | 5.5 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/31267) |


## Defense

### Text (8 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| $R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning | UIUC | main | 7.3 | Spotlight | [ðŸ”—](https://iclr.cc/virtual/2025/poster/30497) |
| HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models | KAIST | main | 7.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27754) |
| RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction | SUNY Stony Brook | main | 6.7 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/30018) |
| Beyond Mere Token Analysis: A Hypergraph Metric Space Framework for Defending Against Socially Engineered LLM Attacks | Fujitsu R&D | main | 6.5 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28172) |
| Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models | N/A | main | 6.2 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28150) |
| Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models | NKU | main | 6.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/30397) |
| Robust LLM safeguarding via refusal feature adversarial training | U of T | main | 5.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28144) |
| On Calibration of LLM-based Guard Models for Reliable Content Moderation | NUS | main | 5.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27843) |


### Vision (1 paper)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks | Fudan | main | 5.5 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/27811) |


## Benchmark

### Text (2 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Certifying Counterfactual Bias in LLMs | N/A | main | 6.2 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/30226) |
| Breach By A Thousand Leaks: Unsafe Information Leakage in 'Safe' AI Responses | N/A | main | 5.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/30768) |


### Agent (2 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Aligned LLMs Are Not Aligned Browser Agents | CMU | main | 7.0 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/29856) |
| AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents | EPFL | main | 6.8 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/32106) |


## Mechanism

### Text (2 papers)

| Title | First Author Affiliation | Track | Average Rating | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|:-----:|
| Safety Alignment Should be Made More Than Just a Few Tokens Deep | Princeton | main | 9.5 | Oral | [ðŸ”—](https://iclr.cc/virtual/2025/poster/30893) |
| Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference | UPenn | main | 6.2 | Poster | [ðŸ”—](https://iclr.cc/virtual/2025/poster/28281) |


