# acl2024 - Jailbreak Research Papers

**Total Papers:** 18

## Content

- [Attack](#Attack)
  - [Text](#Attack-Text)
  - [Hybrid](#Attack-Hybrid)
- [Defense](#Defense)
  - [Text](#Defense-Text)
- [Benchmark](#Benchmark)
  - [Text](#Benchmark-Text)
  - [Vision](#Benchmark-Vision)
  - [Agent](#Benchmark-Agent)

---

## Attack

### Text 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs | UW | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.809/) |
| How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs | VT | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.773/) |
| Jailbreak Open-Sourced Large Language Models via Enforced Decoding | PSU | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.299/) |
| PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails | UM | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.591/) |
| On the Vulnerability of Safety Alignment in Open-Access LLMs | USTC | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.549/) |
| Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues | N/A | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.304/) |
| Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models | IIT Kharagpur | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.960/) |
| TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification | N/A | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.683/) |
| The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance | USC | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.275/) |


### Hybrid 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| SpeechGuard: Exploring the Adversarial Robustness of Multi-modal Large Language Models | Amazon | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.596/) |


## Defense

### Text 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM | PSU | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.568/) |
| Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization | THU | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.481/) |
| GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis | HKUST | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.30/) |
| SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding | UW | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.303/) |
| Defending LLMs against Jailbreaking Attacks via Backtranslation | UCLA | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.948/) |


## Benchmark

### Text 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models | UNSW | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.443/) |


### Vision 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| Red Teaming Visual Language Models | HKU | main | Findings | [ðŸ”—](https://aclanthology.org/2024.findings-acl.198/) |


### Agent 

| Title | First Author Affiliation | Track | Status | Link |
|--------|:-----:|:-----:|:-----:|:-----:|
| ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages | Fudan | main | Long | [ðŸ”—](https://aclanthology.org/2024.acl-long.119/) |


